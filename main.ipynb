{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. Natural Lenguage Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speead up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "x= data['text']\n",
    "y=data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SnowballStemmer and stopwords\n",
    "nltk.download('stopwords')\n",
    "snowball = SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n",
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def clean_text(html_content):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    cleaned_html = re.sub(r'<script.*?>.*?</script>', '', html_content, flags=re.DOTALL)\n",
    "    cleaned_html = re.sub(r'<style.*?>.*?</style>', '', cleaned_html, flags=re.DOTALL)\n",
    "\n",
    "    # Step 2: Remove HTML comments\n",
    "    cleaned_html = re.sub(r'<!--.*?-->', '', cleaned_html, flags=re.DOTALL)\n",
    "\n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(cleaned_html, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the training and test data\n",
    "x_train_cleaned = x_train.apply(clean_text)\n",
    "x_test_cleaned = x_test.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n",
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def data_preprocessing(text):\n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def further_clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    cleaned_text = ' '.join(word for word in words if word not in stop_words)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_text = ' '.join(snowball.stem(word) for word in cleaned_text.split())\n",
    "    \n",
    "    return stemmed_text\n",
    "\n",
    "# Apply the cleaning functions to the training and test data\n",
    "x_train_cleaned = x_train.apply(clean_text).apply(data_preprocessing).apply(further_clean_text)\n",
    "x_test_cleaned = x_test.apply(clean_text).apply(data_preprocessing).apply(further_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n",
      "C:\\Users\\anama\\AppData\\Local\\Temp\\ipykernel_13056\\2221887919.py:11: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(cleaned_html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def further_clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    cleaned_text = ' '.join(word for word in words if word not in stop_words)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_text = ' '.join(snowball.stem(word) for word in cleaned_text.split())\n",
    "    \n",
    "    return stemmed_text\n",
    "\n",
    "x_train_cleaned = x_train.apply(clean_text).apply(further_clean_text)\n",
    "x_test_cleaned = x_test.apply(clean_text).apply(further_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join cleaned words back into a sentence\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply lemmatization to the cleaned text\n",
    "x_train_cleaned = x_train.apply(lemmatize_text)\n",
    "x_test_cleaned = x_test.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words for Training Data:\n",
      "     00  000  000000  00000e25  00000e251  00000eur  000066  0000ff  000m  \\\n",
      "0     0    0       0         0          0         0       0       0     0   \n",
      "1     0    0       0         0          0         0       0       0     0   \n",
      "2     0    0       0         0          0         0       0       0     0   \n",
      "3     0    0       0         0          0         0       0       0     0   \n",
      "4     0    0       0         0          0         0       0       0     0   \n",
      "..   ..  ...     ...       ...        ...       ...     ...     ...   ...   \n",
      "795   0    0       0         0          0         0       0       0     0   \n",
      "796   0    0       0         0          0         0       0       0     0   \n",
      "797   0    0       0         0          0         0       0       0     0   \n",
      "798   0    0       0         0          0         0       0       0     0   \n",
      "799   0    0       0         0          0         0       0       0     0   \n",
      "\n",
      "     000million  ...  â½s  â½t  â½ta  â½te  â½tica  â½to  â½trangers  â½x60ã  \\\n",
      "0             0  ...    0    0     0     0       0     0           0       0   \n",
      "1             0  ...    0    0     0     0       0     0           0       0   \n",
      "2             0  ...    0    0     0     0       0     0           0       0   \n",
      "3             0  ...    0    0     0     0       0     0           0       0   \n",
      "4             0  ...    0    0     0     0       0     0           0       0   \n",
      "..          ...  ...  ...  ...   ...   ...     ...   ...         ...     ...   \n",
      "795           0  ...    0    0     0     0       0     0           0       0   \n",
      "796           0  ...    0    0     0     0       0     0           0       0   \n",
      "797           0  ...    0    0     0     0       0     0           0       0   \n",
      "798           0  ...    0    0     0     0       0     0           0       0   \n",
      "799           0  ...    0    0     0     0       0     0           0       0   \n",
      "\n",
      "     â½x70  â½ã  \n",
      "0        0    0  \n",
      "1        0    0  \n",
      "2        0    0  \n",
      "3        0    0  \n",
      "4        0    0  \n",
      "..     ...  ...  \n",
      "795      0    0  \n",
      "796      0    0  \n",
      "797      0    0  \n",
      "798      0    0  \n",
      "799      0    0  \n",
      "\n",
      "[800 rows x 23057 columns]\n",
      "\n",
      "Bag of Words for Test Data:\n",
      "     00  000  000000  00000e25  00000e251  00000eur  000066  0000ff  000m  \\\n",
      "0     0    0       0         0          0         0       0       0     0   \n",
      "1     0    0       0         0          0         0       0       0     0   \n",
      "2     0    0       0         0          0         0       0       0     0   \n",
      "3     0    0       0         0          0         0       0       0     0   \n",
      "4     0    0       0         0          0         0       0       0     0   \n",
      "..   ..  ...     ...       ...        ...       ...     ...     ...   ...   \n",
      "195   0    0       0         0          0         0       0       0     0   \n",
      "196   0    0       0         0          0         0       0       0     0   \n",
      "197   0    0       0         0          0         0       0       0     0   \n",
      "198   0    0       0         0          0         0       0       0     0   \n",
      "199   0    0       0         0          0         0       0       0     0   \n",
      "\n",
      "     000million  ...  â½s  â½t  â½ta  â½te  â½tica  â½to  â½trangers  â½x60ã  \\\n",
      "0             0  ...    0    0     0     0       0     0           0       0   \n",
      "1             0  ...    0    0     0     0       0     0           0       0   \n",
      "2             0  ...    0    0     0     0       0     0           0       0   \n",
      "3             0  ...    0    0     0     0       0     0           0       0   \n",
      "4             0  ...    0    0     0     0       0     0           0       0   \n",
      "..          ...  ...  ...  ...   ...   ...     ...   ...         ...     ...   \n",
      "195           0  ...    0    0     0     0       0     0           0       0   \n",
      "196           0  ...    0    0     0     0       0     0           0       0   \n",
      "197           0  ...    0    0     0     0       0     0           0       0   \n",
      "198           0  ...    0    0     0     0       0     0           0       0   \n",
      "199           0  ...    0    0     0     0       0     0           0       0   \n",
      "\n",
      "     â½x70  â½ã  \n",
      "0        0    0  \n",
      "1        0    0  \n",
      "2        0    0  \n",
      "3        0    0  \n",
      "4        0    0  \n",
      "..     ...  ...  \n",
      "195      0    0  \n",
      "196      0    0  \n",
      "197      0    0  \n",
      "198      0    0  \n",
      "199      0    0  \n",
      "\n",
      "[200 rows x 23057 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer to create bag of words\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform training data\n",
    "x_train_bow = vectorizer.fit_transform(x_train_cleaned)\n",
    "\n",
    "# Transform test data using the same vectorizer\n",
    "x_test_bow = vectorizer.transform(x_test_cleaned)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert bow matrices to pandas DataFrames for visualization\n",
    "x_train_bow_df = pd.DataFrame(x_train_bow.toarray(), columns=feature_names)\n",
    "x_test_bow_df = pd.DataFrame(x_test_bow.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the bag of words DataFrames\n",
    "print(\"Bag of Words for Training Data:\")\n",
    "print(x_train_bow_df)\n",
    "print(\"\\nBag of Words for Test Data:\")\n",
    "print(x_test_bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Vectorized Training Dataset: (800, 23057)\n",
      "Shape of Vectorized Test Dataset: (200, 23057)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Initialize TfidfVectorizer to create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform training data\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train_cleaned)\n",
    "\n",
    "# Transform test data using the fitted vectorizer\n",
    "x_test_tfidf = vectorizer.transform(x_test_cleaned)\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(\"Shape of Vectorized Training Dataset:\", x_train_tfidf.shape)\n",
    "print(\"Shape of Vectorized Test Dataset:\", x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       125\n",
      "           1       1.00      0.95      0.97        75\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.98      0.97      0.98       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Initialize the classifier (Logistic Regression in this case)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier on the TF-IDF transformed training data\n",
    "classifier.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = classifier.predict(x_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print classification report (optional)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
